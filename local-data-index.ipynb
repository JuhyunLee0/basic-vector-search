{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search - Creating and Updating index from local files\n",
    "\n",
    "This code demonstrates how to use Azure AI Search by using the push API to insert vectors into your search index:\n",
    "\n",
    "+ Create an index schema\n",
    "+ Load the sample data from a local folder\n",
    "+ Embed the documents in-memory using Azure OpenAI's text-embedding-ada-002 model\n",
    "+ Index the vector and nonvector fields on Azure AI Search\n",
    "+ Run a series of vector and hybrid queries, including metadata filtering and hybrid (text + vectors) search. \n",
    "\n",
    "The code uses Azure OpenAI to generate embeddings for title and content fields. You'll need access to Azure OpenAI to run this demo.\n",
    "\n",
    "The code reads the `pdf` files inside `data` folder, which contains the input data for which embeddings need to be generated.\n",
    "\n",
    "The output is a combination of human-readable text and embeddings that can be pushed into a search index.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ An Azure subscription, with [access to Azure OpenAI](https://aka.ms/oai/access). You must have the Azure OpenAI service name and an API key.\n",
    "\n",
    "+ A deployment of the text-embedding-ada-002 embedding model.\n",
    "\n",
    "+ Azure AI Search, any tier, but choose a service that has sufficient capacity for your vector index. We recommend Basic or higher. [Enable semantic ranking](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable) if you want to run the hybrid query with semantic ranking.\n",
    "\n",
    "+ fill out the `.env` file with correct values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables\n",
    "\n",
    "<span style=\"color:red\">make sure to change the `local.env` with `.env`</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import AzureOpenAI\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "\n",
    "load_dotenv(\"./local.env\", override=True) # change this to .env for your own environment\n",
    "\n",
    "# The following variables from your .env file are used in this notebook\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"])\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"] if len(os.environ[\"AZURE_OPENAI_KEY\"]) > 0 else None\n",
    "azure_openai_embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
    "azure_openai_api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "# Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key\n",
    ")\n",
    "\n",
    "# Initialize the SearchClient\n",
    "search_client = SearchClient(\n",
    "    endpoint=endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=credential\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data & Create embeddings\n",
    "Read your data, generate OpenAI embeddings and export to a format to insert your Azure AI Search index:\n",
    "\n",
    "you may run into issue where embedding may time out due to large amount of calls if data contains alot of pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    pdf_reader = PdfReader(file_path)\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    chunks = []\n",
    "    for i in range(0, num_pages, 5):\n",
    "        chunk_text = \"\"\n",
    "        for page in pdf_reader.pages[i:min(i + 5, num_pages)]:\n",
    "            chunk_text += page.extract_text()\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# Function to vectorize text using Azure Text Analytics\n",
    "async def vectorize_text(text):\n",
    "    content_response = client.embeddings.create(input=text, model=azure_openai_embedding_deployment)\n",
    "    return content_response.data[0].embedding\n",
    "\n",
    "data_folder = \"data\"\n",
    "for file_name in os.listdir(data_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        chunks = process_pdf(file_path)\n",
    "        print(len(chunks))\n",
    "        file_chunks = []\n",
    "        for chunk in chunks:\n",
    "            vector = await vectorize_text(chunk)\n",
    "            file_chunks.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"title\": file_name,\n",
    "                \"content\": chunk,\n",
    "                \"contentVector\": vector\n",
    "            })\n",
    "        # Output embeddings to docVectors.json file\n",
    "        output_path = os.path.join('.', 'output', f\"{file_name}.json\")\n",
    "        output_directory = os.path.dirname(output_path)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(file_chunks, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "\n",
    "Create your search index schema and vector search configuration. If you get an error, check the search service for available quota and check the .env file to make sure you're using a unique search index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " local-test-index created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into index\n",
    "take all the `.json` file from output and update the index with its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 22 documents in total\n",
      "Uploaded 21 documents in total\n",
      "Documents updated successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing the JSON files\n",
    "json_directory = \"output\"\n",
    "\n",
    "# Load and update documents from JSON files\n",
    "for filename in os.listdir(json_directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_directory, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            document = json.load(file)\n",
    "            # Use SearchIndexingBufferedSender to upload the documents in batches optimized for indexing  \n",
    "            with SearchIndexingBufferedSender(  \n",
    "                endpoint=endpoint,  \n",
    "                index_name=index_name,  \n",
    "                credential=credential,  \n",
    "            ) as batch_client:  \n",
    "                # Add upload actions for all documents  \n",
    "                batch_client.upload_documents(documents=document) \n",
    "            print(f\"Uploaded {len(document)} documents in total\")\n",
    "print(\"Documents updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">it may take couple of minutes for the index to populate with batched data.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"deductable for health plus plan\"  \n",
    "  \n",
    "embedding = client.embeddings.create(input=query, model=azure_openai_embedding_deployment).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a pure vector search to demonstrate OpenAI's text-embedding-ada-002 multilingual capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search multi-lingual (e.g 'tools for software development' in Dutch)  \n",
    "query = \"deductable for health plus plan\"  \n",
    "  \n",
    "embedding = client.embeddings.create(input=query, model=azure_openai_embedding_deployment).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform an Exhaustive KNN exact nearest neighbor search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how you can exhaustively search your vector index regardless of what index you have, HNSW or ExhaustiveKNN. You can use this to calculate the ground-truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"deductable for health plus plan\"  \n",
    "  \n",
    "embedding = client.embeddings.create(input=query, model=azure_openai_embedding_deployment).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Pure Vector Search with a filter\n",
    "This example shows how to apply filters on your index. Note, that you can choose whether you want to use Pre-Filtering (default) or Post-Filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorFilterMode\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"deductable for health plus plan\"  \n",
    "  \n",
    "embedding = client.embeddings.create(input=query, model=azure_openai_embedding_deployment).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    vector_filter_mode=VectorFilterMode.PRE_FILTER,\n",
    "    filter=\"title eq 'Northwind_Health_Plus_Benefits_Details.pdf'\",\n",
    "    select=[\"title\", \"content\"],\n",
    ")\n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "query = \"deductable for health plus plan\"  \n",
    "  \n",
    "embedding = client.embeddings.create(input=query, model=azure_openai_embedding_deployment).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    "    top=3\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Semantic Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
    "\n",
    "# Semantic Hybrid Search\n",
    "query = \"deductable for health plus plan\"\n",
    "\n",
    "embedding = client.embeddings.create(input=query, model=azure_openai_embedding_deployment).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\"],\n",
    "    query_type=QueryType.SEMANTIC, semantic_configuration_name='my-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
